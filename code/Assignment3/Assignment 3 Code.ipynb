{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 3\n",
    "# Jian Zhang, student ID: 219012058\n",
    "# Date: 19.11.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "import cv2 # use opencv to read image\n",
    "import matplotlib.pyplot as plt # use matplotlib to show image\n",
    "from random import randint\n",
    "from skimage import morphology\n",
    "from math import ceil, floor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Texture\n",
    "# implement the texture synthesis algorithm\n",
    "# modified by https://github.com/goldbema/TextureSynthesis/blob/master/synthesis.py\n",
    "# algorithm details: https://people.eecs.berkeley.edu/~efros/research/EfrosLeung.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIGHT_CONNECTED_NEIGHBOR_KERNEL = np.array([[1., 1., 1.],\n",
    "                                            [1., 0., 1.],\n",
    "                                            [1., 1., 1.]], dtype=np.float64)\n",
    "SIGMA_COEFF = 6.4      # The denominator for a 2D Gaussian sigma used in the reference implementation.\n",
    "ERROR_THRESHOLD = 0.1  # The default error threshold for synthesis acceptance in the reference implementation.\n",
    "\n",
    "\n",
    "def normalized_ssd(sample, window, mask):\n",
    "    wh, ww = window.shape\n",
    "    sh, sw = sample.shape\n",
    "\n",
    "    # Get sliding window views of the sample, window, and mask.\n",
    "    strided_sample = np.lib.stride_tricks.as_strided(sample, shape=((sh-wh+1), (sw-ww+1), wh, ww), \n",
    "                        strides=(sample.strides[0], sample.strides[1], sample.strides[0], sample.strides[1]))\n",
    "    strided_sample = strided_sample.reshape(-1, wh, ww)\n",
    "\n",
    "    # Note that the window and mask views have the same shape as the strided sample, but the kernel is fixed\n",
    "    # rather than sliding for each of these components.\n",
    "    strided_window = np.lib.stride_tricks.as_strided(window, shape=((sh-wh+1), (sw-ww+1), wh, ww),\n",
    "                        strides=(0, 0, window.strides[0], window.strides[1]))\n",
    "    strided_window = strided_window.reshape(-1, wh, ww)\n",
    "\n",
    "    strided_mask = np.lib.stride_tricks.as_strided(mask, shape=((sh-wh+1), (sw-ww+1), wh, ww),\n",
    "                        strides=(0, 0, mask.strides[0], mask.strides[1]))\n",
    "    strided_mask = strided_mask.reshape(-1, wh, ww)\n",
    "\n",
    "    # Form a 2D Gaussian weight matrix from symmetric linearly separable Gaussian kernels and generate a \n",
    "    # strided view over this matrix.\n",
    "    sigma = wh / SIGMA_COEFF\n",
    "    kernel = cv2.getGaussianKernel(ksize=wh, sigma=sigma)\n",
    "    kernel_2d = kernel * kernel.T\n",
    "\n",
    "    strided_kernel = np.lib.stride_tricks.as_strided(kernel_2d, shape=((sh-wh+1), (sw-ww+1), wh, ww),\n",
    "                        strides=(0, 0, kernel_2d.strides[0], kernel_2d.strides[1]))\n",
    "    strided_kernel = strided_kernel.reshape(-1, wh, ww)\n",
    "\n",
    "    # Take the sum of squared differences over all sliding sample windows and weight it so that only existing neighbors\n",
    "    # contribute to error. Use the Gaussian kernel to weight central values more strongly than distant neighbors.\n",
    "    squared_differences = ((strided_sample - strided_window)**2) * strided_kernel * strided_mask\n",
    "    ssd = np.sum(squared_differences, axis=(1,2))\n",
    "    ssd = ssd.reshape(sh-wh+1, sw-ww+1)\n",
    "\n",
    "    # Normalize the SSD by the maximum possible contribution.\n",
    "    total_ssd = np.sum(mask * kernel_2d)\n",
    "    normalized_ssd = ssd / total_ssd\n",
    "\n",
    "    return normalized_ssd\n",
    "\n",
    "def get_candidate_indices(normalized_ssd, error_threshold=ERROR_THRESHOLD):\n",
    "    min_ssd = np.min(normalized_ssd)\n",
    "    min_threshold = min_ssd * (1. + error_threshold)\n",
    "    indices = np.where(normalized_ssd <= min_threshold)\n",
    "    return indices\n",
    "\n",
    "def select_pixel_index(normalized_ssd, indices, method='uniform'):\n",
    "    N = indices[0].shape[0]\n",
    "\n",
    "    if method == 'uniform':\n",
    "        weights = np.ones(N) / float(N)\n",
    "    else:\n",
    "        weights = normalized_ssd[indices]\n",
    "        weights = weights / np.sum(weights)\n",
    "\n",
    "    # Select a random pixel index from the index list.\n",
    "    selection = np.random.choice(np.arange(N), size=1, p=weights)\n",
    "    selected_index = (indices[0][selection], indices[1][selection])\n",
    "    \n",
    "    return selected_index\n",
    "\n",
    "def get_neighboring_pixel_indices(pixel_mask):\n",
    "    # Taking the difference between the dilated mask and the initial mask\n",
    "    # gives only the 8-connected neighbors of the mask frontier.\n",
    "    kernel = np.ones((3,3))\n",
    "    dilated_mask = cv2.dilate(pixel_mask, kernel, iterations=1)\n",
    "    neighbors = dilated_mask - pixel_mask\n",
    "\n",
    "    # Recover the indices of the mask frontier.\n",
    "    neighbor_indices = np.nonzero(neighbors)\n",
    "\n",
    "    return neighbor_indices\n",
    "\n",
    "def permute_neighbors(pixel_mask, neighbors):\n",
    "    N = neighbors[0].shape[0]\n",
    "\n",
    "    # Generate a permutation of the neigboring indices\n",
    "    permuted_indices = np.random.permutation(np.arange(N))\n",
    "    permuted_neighbors = (neighbors[0][permuted_indices], neighbors[1][permuted_indices])\n",
    "\n",
    "    # Use convolution to count the number of existing neighbors for all entries in the mask.\n",
    "    neighbor_count = cv2.filter2D(pixel_mask, ddepth=-1, kernel=EIGHT_CONNECTED_NEIGHBOR_KERNEL, borderType=cv2.BORDER_CONSTANT)\n",
    "\n",
    "    # Sort the permuted neighboring indices by quantity of existing neighbors descending.\n",
    "    permuted_neighbor_counts = neighbor_count[permuted_neighbors]\n",
    "\n",
    "    sorted_order = np.argsort(permuted_neighbor_counts)[::-1]\n",
    "    permuted_neighbors = (permuted_neighbors[0][sorted_order], permuted_neighbors[1][sorted_order])\n",
    "\n",
    "    return permuted_neighbors\n",
    "\n",
    "def texture_can_be_synthesized(mask):\n",
    "    # The texture can be synthesized while the mask has unfilled entries.\n",
    "    mh, mw = mask.shape[:2]\n",
    "    num_completed = np.count_nonzero(mask)\n",
    "    num_incomplete = (mh * mw) - num_completed\n",
    "    \n",
    "    return num_incomplete > 0\n",
    "\n",
    "def initialize_texture_synthesis(original_sample, window_size, kernel_size):\n",
    "\n",
    "    # Convert original to sample representation.\n",
    "    sample = cv2.cvtColor(original_sample, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Convert sample to floating point and normalize to the range [0., 1.]\n",
    "    sample = sample.astype(np.float64)\n",
    "    sample = sample / 255.\n",
    "\n",
    "    # Generate window\n",
    "    window = np.zeros(window_size, dtype=np.float64)\n",
    "\n",
    "    # Generate output window\n",
    "    if original_sample.ndim == 2:\n",
    "        result_window = np.zeros_like(window, dtype=np.uint8)\n",
    "    else:\n",
    "        result_window = np.zeros(window_size + (3,), dtype=np.uint8)\n",
    "\n",
    "    # Generate window mask\n",
    "    h, w = window.shape\n",
    "    mask = np.zeros((h, w), dtype=np.float64)\n",
    "\n",
    "    # Initialize window with random seed from sample\n",
    "    sh, sw = original_sample.shape[:2]\n",
    "    ih = np.random.randint(sh-3+1)\n",
    "    iw = np.random.randint(sw-3+1)\n",
    "    seed = sample[ih:ih+3, iw:iw+3]\n",
    "\n",
    "    # Place seed in center of window\n",
    "    ph, pw = (h//2)-1, (w//2)-1\n",
    "    window[ph:ph+3, pw:pw+3] = seed\n",
    "    mask[ph:ph+3, pw:pw+3] = 1\n",
    "    result_window[ph:ph+3, pw:pw+3] = original_sample[ih:ih+3, iw:iw+3]\n",
    "\n",
    "    # Obtain padded versions of window and mask\n",
    "    win = kernel_size//2\n",
    "    padded_window = cv2.copyMakeBorder(window, \n",
    "                                       top=win, bottom=win, left=win, right=win, borderType=cv2.BORDER_CONSTANT, value=0.)\n",
    "    padded_mask = cv2.copyMakeBorder(mask,\n",
    "                                     top=win, bottom=win, left=win, right=win, borderType=cv2.BORDER_CONSTANT, value=0.)\n",
    "    \n",
    "    # Obtain views of the padded window and mask\n",
    "    window = padded_window[win:-win, win:-win]\n",
    "    mask = padded_mask[win:-win, win:-win]\n",
    "\n",
    "    return sample, window, mask, padded_window, padded_mask, result_window\n",
    "    \n",
    "def synthesize_texture(original_sample, window_size, kernel_size):\n",
    "    \n",
    "    (sample, window, mask, padded_window, \n",
    "        padded_mask, result_window) = initialize_texture_synthesis(original_sample, window_size, kernel_size)\n",
    "\n",
    "    # Synthesize texture until all pixels in the window are filled.\n",
    "    while texture_can_be_synthesized(mask):\n",
    "        # Get neighboring indices\n",
    "        neighboring_indices = get_neighboring_pixel_indices(mask)\n",
    "\n",
    "        # Permute and sort neighboring indices by quantity of 8-connected neighbors.\n",
    "        neighboring_indices = permute_neighbors(mask, neighboring_indices)\n",
    "        \n",
    "        for ch, cw in zip(neighboring_indices[0], neighboring_indices[1]):\n",
    "\n",
    "            window_slice = padded_window[ch:ch+kernel_size, cw:cw+kernel_size]\n",
    "            mask_slice = padded_mask[ch:ch+kernel_size, cw:cw+kernel_size]\n",
    "\n",
    "            # Compute SSD for the current pixel neighborhood and select an index with low error.\n",
    "            ssd = normalized_ssd(sample, window_slice, mask_slice)\n",
    "            indices = get_candidate_indices(ssd)\n",
    "            selected_index = select_pixel_index(ssd, indices)\n",
    "\n",
    "            # Translate index to accommodate padding.\n",
    "            selected_index = (selected_index[0] + kernel_size // 2, selected_index[1] + kernel_size // 2)\n",
    "\n",
    "            # Set windows and mask.\n",
    "            window[ch, cw] = sample[selected_index]\n",
    "            mask[ch, cw] = 1\n",
    "            result_window[ch, cw] = original_sample[selected_index[0], selected_index[1]]\n",
    "\n",
    "    return result_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the image 'brick.png' to synthesize a 1000*1000 image\n",
    "\n",
    "img_brick = cv2.imread('brick.png') \n",
    "brick_height = img_brick.shape[0]\n",
    "brick_width  = img_brick.shape[1]\n",
    "output_height = 1000\n",
    "output_width  = 1000\n",
    "PatchSize  = 50 # 88 is the solution but it cannot be divisible by 1000, 50 is good enough\n",
    "\n",
    "synthesized_texture = synthesize_texture(original_sample=img_brick, \n",
    "                                         window_size=(output_height, output_width), \n",
    "                                         kernel_size=PatchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output synthesized texture\n",
    "cv2.imwrite('synthesized.jpg', synthesized_texture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Segmentation\n",
    "img_dayun = cv2.imread('dayun.jpg') \n",
    "img_tanglang = cv2.imread('tanglang.jpg') \n",
    "# Convert to floats instead of the default 8 bits integer coding. \n",
    "# Dividing by 255 is important so that plt.imshow behaves works well on float data\n",
    "# need to be in the range [0-1]\n",
    "# img_dayun = np.array(img_dayun, dtype=np.float64) / 255\n",
    "# img_tanglang = np.array(img_tanglang, dtype=np.float64) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayun_height = img_dayun.shape[0]\n",
    "dayun_width  = img_dayun.shape[1]\n",
    "dayun_array = np.reshape(img_dayun, (dayun_height * dayun_width, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, dayun_height)\n",
    "y = np.arange(0, dayun_width)\n",
    "xx, yy = np.meshgrid(y, x)\n",
    "xxx, yyy = xx.flatten(), yy.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data vector, select proper weights\n",
    "dayun_feature = np.c_[dayun_array / 255, yyy / dayun_height, xxx / dayun_width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayun_sample = shuffle(dayun_feature, random_state=0)[:100000]\n",
    "kmeans = KMeans(n_clusters=5, n_jobs=-1).fit(dayun_sample)\n",
    "labels = kmeans.predict(dayun_feature)\n",
    "kmeans_dayun = labels.reshape([dayun_height, dayun_width])\n",
    "cv2.imwrite('kmeans_dayun.jpg', kmeans_dayun * int(256 / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanglang_height = img_tanglang.shape[0]\n",
    "tanglang_width  = img_tanglang.shape[1]\n",
    "tanglang_array = np.reshape(img_tanglang, (tanglang_height * tanglang_width, 3))\n",
    "x = np.arange(0, tanglang_height)\n",
    "y = np.arange(0, tanglang_width)\n",
    "xx, yy = np.meshgrid(y, x)\n",
    "xxx, yyy = xx.flatten(), yy.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data vector, select proper weights\n",
    "tanglang_feature = np.c_[tanglang_array / 255, yyy / tanglang_height, xxx / tanglang_width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanglang_sample = shuffle(tanglang_feature, random_state=0)[:100000]\n",
    "kmeans = KMeans(n_clusters=5, n_jobs=-1).fit(tanglang_sample)\n",
    "tanglang_label = kmeans.predict(tanglang_feature)\n",
    "kmeans_tanglang = tanglang_label.reshape([tanglang_height, tanglang_width])\n",
    "cv2.imwrite('kmeans_tanglang.jpg', kmeans_tanglang * int(256 / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Face Detection\n",
    "# modified from https://github.com/Simon-Hohberg/Viola-Jones\n",
    "\n",
    "# turn a image array into integral image, return type ndarray\n",
    "def to_integral_image(img_arr):\n",
    "\n",
    "    row_sum = np.zeros(img_arr.shape)\n",
    "    # we need an additional column and row\n",
    "    integral_image_arr = np.zeros((img_arr.shape[0] + 1, img_arr.shape[1] + 1))\n",
    "    for x in range(img_arr.shape[1]):\n",
    "        for y in range(img_arr.shape[0]):\n",
    "            row_sum[y, x] = row_sum[y-1, x] + img_arr[y, x]\n",
    "            integral_image_arr[y+1, x+1] = integral_image_arr[y+1, x-1+1] + row_sum[y, x]\n",
    "    return integral_image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_region(integral_img_arr, top_left, bottom_right):\n",
    "    \"\"\"\n",
    "    Calculates the sum in the rectangle specified by the given tuples.\n",
    "    :param integral_img_arr:\n",
    "    :type integral_img_arr: numpy.ndarray\n",
    "    :param top_left: (x, y) of the rectangle's top left corner\n",
    "    :type top_left: (int, int)\n",
    "    :param bottom_right: (x, y) of the rectangle's bottom right corner\n",
    "    :type bottom_right: (int, int)\n",
    "    :return The sum of all pixels in the given rectangle\n",
    "    :rtype int\n",
    "    \"\"\"\n",
    "    # swap tuples\n",
    "    top_left = (top_left[1], top_left[0])\n",
    "    bottom_right = (bottom_right[1], bottom_right[0])\n",
    "    if top_left == bottom_right:\n",
    "        return integral_img_arr[top_left]\n",
    "    top_right = (bottom_right[0], top_left[1])\n",
    "    bottom_left = (top_left[0], bottom_right[1])\n",
    "    return integral_img_arr[bottom_right] - integral_img_arr[top_right] - integral_img_arr[bottom_left] + integral_img_arr[top_left]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path):\n",
    "    images = []\n",
    "    for _file in os.listdir(path):\n",
    "        if _file.endswith('.jpg') or _file.endswith('pgm'):\n",
    "            img_arr = np.array(Image.open((os.path.join(path, _file))).convert('L'), dtype=np.float64)\n",
    "            img_arr /= img_arr.max()\n",
    "            images.append(img_arr)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lfw images and nonface images\n",
    "nonface_path = './face_detection/nonface'\n",
    "lfw_path = './face_detection/lfw1000'\n",
    "nonface_images = load_images(nonface_path)\n",
    "lfw_images = load_images(lfw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum(**enums):\n",
    "    return type('Enum', (), enums)\n",
    "\n",
    "FeatureType = enum(TWO_VERTICAL=(1, 2), TWO_HORIZONTAL=(2, 1), THREE_HORIZONTAL=(3, 1), THREE_VERTICAL=(1, 3), FOUR=(2, 2))\n",
    "FeatureTypes = [FeatureType.TWO_VERTICAL, FeatureType.TWO_HORIZONTAL, FeatureType.THREE_VERTICAL, FeatureType.THREE_HORIZONTAL, FeatureType.FOUR]\n",
    "\n",
    "\n",
    "class HaarLikeFeature(object):\n",
    "    \"\"\"\n",
    "    Class representing a haar-like feature.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_type, position, width, height, threshold, polarity):\n",
    "        \"\"\"\n",
    "        Creates a new haar-like feature.\n",
    "        :param feature_type: Type of new feature, see FeatureType enum\n",
    "        :type feature_type: violajonse.HaarLikeFeature.FeatureTypes\n",
    "        :param position: Top left corner where the feature begins (x, y)\n",
    "        :type position: (int, int)\n",
    "        :param width: Width of the feature\n",
    "        :type width: int\n",
    "        :param height: Height of the feature\n",
    "        :type height: int\n",
    "        :param threshold: Feature threshold\n",
    "        :type threshold: float\n",
    "        :param polarity: polarity of the feature -1 or 1\n",
    "        :type polarity: int\n",
    "        \"\"\"\n",
    "        self.type = feature_type\n",
    "        self.top_left = position\n",
    "        self.bottom_right = (position[0] + width, position[1] + height)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.threshold = threshold\n",
    "        self.polarity = polarity\n",
    "        self.weight = 1\n",
    "    \n",
    "    def get_score(self, int_img):\n",
    "        \"\"\"\n",
    "        Get score for given integral image array.\n",
    "        :param int_img: Integral image array\n",
    "        :type int_img: numpy.ndarray\n",
    "        :return: Score for given feature\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        if self.type == FeatureType.TWO_VERTICAL:\n",
    "            first = sum_region(int_img, self.top_left, (self.top_left[0] + self.width, int(self.top_left[1] + self.height / 2)))\n",
    "            second = sum_region(int_img, (self.top_left[0], int(self.top_left[1] + self.height / 2)), self.bottom_right)\n",
    "            score = first - second\n",
    "        elif self.type == FeatureType.TWO_HORIZONTAL:\n",
    "            first = sum_region(int_img, self.top_left, (int(self.top_left[0] + self.width / 2), self.top_left[1] + self.height))\n",
    "            second = sum_region(int_img, (int(self.top_left[0] + self.width / 2), self.top_left[1]), self.bottom_right)\n",
    "            score = first - second\n",
    "        elif self.type == FeatureType.THREE_HORIZONTAL:\n",
    "            first = sum_region(int_img, self.top_left, (int(self.top_left[0] + self.width / 3), self.top_left[1] + self.height))\n",
    "            second = sum_region(int_img, (int(self.top_left[0] + self.width / 3), self.top_left[1]), (int(self.top_left[0] + 2 * self.width / 3), self.top_left[1] + self.height))\n",
    "            third = sum_region(int_img, (int(self.top_left[0] + 2 * self.width / 3), self.top_left[1]), self.bottom_right)\n",
    "            score = first - second + third\n",
    "        elif self.type == FeatureType.THREE_VERTICAL:\n",
    "            first = sum_region(int_img, self.top_left, (self.bottom_right[0], int(self.top_left[1] + self.height / 3)))\n",
    "            second = sum_region(int_img, (self.top_left[0], int(self.top_left[1] + self.height / 3)), (self.bottom_right[0], int(self.top_left[1] + 2 * self.height / 3)))\n",
    "            third = sum_region(int_img, (self.top_left[0], int(self.top_left[1] + 2 * self.height / 3)), self.bottom_right)\n",
    "            score = first - second + third\n",
    "        elif self.type == FeatureType.FOUR:\n",
    "            # top left area\n",
    "            first = sum_region(int_img, self.top_left, (int(self.top_left[0] + self.width / 2), int(self.top_left[1] + self.height / 2)))\n",
    "            # top right area\n",
    "            second = sum_region(int_img, (int(self.top_left[0] + self.width / 2), self.top_left[1]), (self.bottom_right[0], int(self.top_left[1] + self.height / 2)))\n",
    "            # bottom left area\n",
    "            third = sum_region(int_img, (self.top_left[0], int(self.top_left[1] + self.height / 2)), (int(self.top_left[0] + self.width / 2), self.bottom_right[1]))\n",
    "            # bottom right area\n",
    "            fourth = sum_region(int_img, (int(self.top_left[0] + self.width / 2), int(self.top_left[1] + self.height / 2)), self.bottom_right)\n",
    "            score = first - second - third + fourth\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(img_height, img_width, min_feature_width, max_feature_width, min_feature_height, max_feature_height):\n",
    "    features = []\n",
    "    for feature in FeatureTypes:\n",
    "        # FeatureTypes are just tuples\n",
    "        feature_start_width = max(min_feature_width, feature[0])\n",
    "        for feature_width in range(feature_start_width, max_feature_width, feature[0]):\n",
    "            feature_start_height = max(min_feature_height, feature[1])\n",
    "            for feature_height in range(feature_start_height, max_feature_height, feature[1]):\n",
    "                for x in range(img_width - feature_width):\n",
    "                    for y in range(img_height - feature_height):\n",
    "                        features.append(HaarLikeFeature(feature, (x, y), feature_width, feature_height, 0, 1))\n",
    "                        features.append(HaarLikeFeature(feature, (x, y), feature_width, feature_height, 0, -1))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classifiers = 2\n",
    "# For performance reasons restricting feature size\n",
    "min_feature_height = 8\n",
    "max_feature_height = 10\n",
    "min_feature_width = 8\n",
    "max_feature_width = 10\n",
    "\n",
    "img_height, img_width = lfw_images[0].shape\n",
    "features = create_features(img_height, img_width, min_feature_width, max_feature_width, min_feature_height, max_feature_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train positive set\n",
    "train_df = pd.DataFrame()\n",
    "train_pos_images = lfw_images[:100]\n",
    "for img in train_pos_images:\n",
    "    \n",
    "    int_img = to_integral_image(img)\n",
    "    current_features = []\n",
    "    for feature in features:\n",
    "        current_features.append(feature.get_score(int_img))\n",
    "    np_features = np.array(current_features).reshape(1,-1)\n",
    "    tmp_df = pd.DataFrame(np_features)\n",
    "    tmp_df['label'] = 1\n",
    "    train_df = pd.concat([train_df, tmp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train negative set\n",
    "# randomly crop 64*64 size images from original images\n",
    "train_neg_images = []\n",
    "for neg_img in nonface_images[:10]:\n",
    "    count = 0\n",
    "    while count < 10:\n",
    "        h = randint(0, neg_img.shape[0] - img_height)\n",
    "        w = randint(0, neg_img.shape[1] - img_width)\n",
    "        cropImg = neg_img[(h):(h + img_height - 1), (w):(w + img_width - 1)]\n",
    "        int_img = to_integral_image(cropImg)\n",
    "        train_neg_images.append(int_img)\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in train_neg_images:\n",
    "    \n",
    "    int_img = to_integral_image(img)\n",
    "    current_features = []\n",
    "    for feature in features:\n",
    "        current_features.append(feature.get_score(int_img))\n",
    "    np_features = np.array(current_features).reshape(1,-1)\n",
    "    tmp_df = pd.DataFrame(np_features)\n",
    "    tmp_df['label'] = 0\n",
    "    train_df = pd.concat([train_df, tmp_df])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test set for positive image\n",
    "test_pos_df = pd.DataFrame()\n",
    "test_pos_images = lfw_images[100:200]\n",
    "for img in test_pos_images:\n",
    "    \n",
    "    int_img = to_integral_image(img)\n",
    "    current_features = []\n",
    "    for feature in features:\n",
    "        current_features.append(feature.get_score(int_img))\n",
    "    np_features = np.array(current_features).reshape(1,-1)\n",
    "    tmp_df = pd.DataFrame(np_features)\n",
    "    test_pos_df = pd.concat([test_pos_df, tmp_df])\n",
    "test_pos_df = test_pos_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test set for negative image\n",
    "# test_neg_df = pd.DataFrame()\n",
    "# test_neg_images = nonface_images[40:60]\n",
    "# for img in test_neg_images:\n",
    "    \n",
    "#     img_resized = cv2.resize(img, (img_height, img_width))\n",
    "#     int_img = to_integral_image(img_resized)\n",
    "#     current_features = []\n",
    "#     for feature in features:\n",
    "#         current_features.append(feature.get_score(int_img))\n",
    "#     np_features = np.array(current_features).reshape(1,-1)\n",
    "#     tmp_df = pd.DataFrame(np_features)\n",
    "#     test_neg_df = pd.concat([test_neg_df, tmp_df])\n",
    "# test_neg_df = test_neg_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training with adaboost classifier\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(train_df.iloc[:, :-1], train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select strong features for cascaded classifier\n",
    "plt.plot(model.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stored high importance features index\n",
    "maxindex = np.argsort(-model.feature_importances_)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct a cascaded classfier\n",
    "# looking at 2, 10, 50, 200, 2000 features\n",
    "model_features_2 = AdaBoostClassifier()\n",
    "model_features_2.fit(train_df.iloc[:, maxindex[:2]], train_df['label'])\n",
    "model_features_10 = AdaBoostClassifier()\n",
    "model_features_10.fit(train_df.iloc[:, maxindex[:10]], train_df['label'])\n",
    "model_features_50 = AdaBoostClassifier()\n",
    "model_features_50.fit(train_df.iloc[:, maxindex[:50]], train_df['label'])\n",
    "model_features_200 = AdaBoostClassifier()\n",
    "model_features_200.fit(train_df.iloc[:, maxindex[:200]], train_df['label'])\n",
    "model_features_2000 = AdaBoostClassifier()\n",
    "model_features_2000.fit(train_df.iloc[:, maxindex[:2000]], train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use sorted features importance in adaboost\n",
    "# use simple and strong classifier first, then we reject those nonface images\n",
    "# otherwise, we continue to next classifier\n",
    "def cascaded(features):\n",
    "    \n",
    "    label = model_features_2.predict(features[:,maxindex[:2]])\n",
    "    if label == 0:\n",
    "        return 0\n",
    "    label = model_features_10.predict(features[:,maxindex[:10]])\n",
    "    if label == 0:\n",
    "        return 0\n",
    "    label = model_features_50.predict(features[:,maxindex[:50]])\n",
    "    if label == 0:\n",
    "        return 0\n",
    "    label = model_features_200.predict(features[:,maxindex[:200]])\n",
    "    if label == 0:\n",
    "        return 0\n",
    "    label = model_features_2000.predict(features[:,maxindex[:2000]])\n",
    "    if label == 0:\n",
    "        return 0\n",
    "    label = model.predict(features)\n",
    "    if label == 0:\n",
    "        return 0\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on Solvay.jpg and Big3.jpg\n",
    "img_solvay = cv2.imread('./face_detection/Solvay.jpg', 0) \n",
    "img_big3 = cv2.imread('./face_detection/Big3.jpg', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 32\n",
    "h_solvay, w_solvay = img_solvay.shape\n",
    "\n",
    "face_location = []\n",
    "for h in range(0, h_solvay - img_height, stride):\n",
    "    for w in range(0, w_solvay - img_width, stride):\n",
    "        cropImg = img_solvay[(h):(h + img_height - 1), (w):(w + img_width - 1)]\n",
    "        int_img = to_integral_image(cropImg)\n",
    "        current_features = []\n",
    "        for feature in features:\n",
    "            current_features.append(feature.get_score(int_img))\n",
    "        current_features = np.array(current_features).reshape(1,-1)\n",
    "        if cascaded(current_features):\n",
    "            print('find one face, location:', h, w)\n",
    "            face_location.append((h,w))\n",
    "            w += img_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find one face, location: 192 320\n",
      "find one face, location: 192 576\n",
      "find one face, location: 256 544\n",
      "find one face, location: 320 288\n",
      "find one face, location: 352 32\n"
     ]
    }
   ],
   "source": [
    "stride = 32\n",
    "h_big3, w_big3 = img_big3.shape\n",
    "\n",
    "face_location = []\n",
    "for h in range(0, h_big3 - img_height, stride):\n",
    "    for w in range(0, w_big3 - img_width, stride):\n",
    "        cropImg = img_big3[(h):(h + img_height - 1), (w):(w + img_width - 1)]\n",
    "        int_img = to_integral_image(cropImg)\n",
    "        current_features = []\n",
    "        for feature in features:\n",
    "            current_features.append(feature.get_score(int_img))\n",
    "        current_features = np.array(current_features).reshape(1,-1)\n",
    "        if cascaded(current_features):\n",
    "            print('find one face, location:', h, w)\n",
    "            face_location.append((h,w))\n",
    "            w += img_width"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
